

## 1、本地安装自己的大语言模型

> - Ollama 是一个开源的大型语言模型（LLM）服务工具，它允许用户在本地机器上运行和部署大型语言模型。
> - Ollama 设计为一个框架，旨在简化在 Docker 容器中部署和管理大型语言模型的过程，使得这一过程变得简单快捷。
> - 用户可以通过简单的命令行操作，快速在本地运行如 Llama 3 这样的开源大型语言模型。



### 官网下载 ollama 运行大语言模型的软件



> ollama 官网: [前往 ollama](https://ollama.com/)



### 运行模型

> 命令执行的同时，会自动查看当前是否有模型名字为 `llama3` 的大模型，没有的话会自动去内部配置的大模型仓库下载并启动



```bash
$ ollama run llama3		# 尝试下载 llama3(默认回答是英文的) 这个大模型来使用，当然还有很多其他的大模型，大家可以去官网去看看
```



### 安装`千问大模型`(默认支持中文回答)



```bash
$ ollama run qwen		# 安装并运行千问大模型
```



## 2、如何构建自己的 llama3 中文模型



### 下载中文 llama3 大模型文件

> Hugging Face官网: [前往下载 llama3 中文大模型](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-f16/tree/main)



### 编写模型文件(创建 Modelfile 文件)

```md
# FROM 指定 GGUF 文件的路径
FROM F:/models/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf
```



### 创建 ollama 模型

```bash
$ ollama create geek-llama3-8B-Chinese -f ./Modelfile
```



> 这个命令会读取当前路径下创建的`Modelfile`文件中的配置，并创建一个名为 `geek-llama3-8B-Chinese` 的新模型。



### 查看 ollama 当前的模型列表

```bash
$ ollama list
```



### 运行已创建的大模型

```bash
$ ollama run geek-llama3-8B-Chinese:latest
```



## 扩展

>参考：[推荐阅读博客文章](https://mp.weixin.qq.com/s?__biz=MzUyODk0Njc1NQ==&mid=2247486353&idx=1&sn=b590181589be96cf9e016b39cc4cdbf7&chksm=fb64f3917f0dfa282b943c03352c06a6adfe4a4043805be7440cc3ff45f77c16841a1cb15c11&scene=27)

















